```{r setup}
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(knitr)
library(stringr)
```

```{r config}
cumulative_experiment_name <- "cumulative_gains_study_aberth"
multithreading_experiment_name <- "multithreading"
step_size_experiment_name <- "step_size_fixed"
step_size_experiment_fixed_name <- "step_size_fixed_fixed"
root_finding_experiment_name <- "root_finding"
results_dir <- "./simulations"
```

```{r data_loading_functions}
# Function to load data - reusable across experiments
load_experiment_data <- function(experiment_name, results_dir) {
  timing_file <- file.path(results_dir, paste0(experiment_name, "_timing.csv"))
  estimates_file <- file.path(results_dir, paste0(experiment_name, "_estimates.csv"))
  validation_file <- file.path(results_dir, paste0(experiment_name, "_validation.csv"))
  
  cat("Reading results from experiment:", experiment_name, "\n")
  
  # Read data
  timing_data <- read_csv(timing_file, show_col_types = FALSE)
  estimates_data <- read_csv(estimates_file, show_col_types = FALSE)
  validation_data <- read_csv(validation_file, show_col_types = FALSE)
  
  cat("Data loaded successfully:\n")
  cat("  Timing entries:", nrow(timing_data), "\n")
  cat("  Estimation entries:", nrow(estimates_data), "\n")
  cat("  Validation entries:", nrow(validation_data), "\n\n")
  
  return(list(
    timing = timing_data,
    estimates = estimates_data,
    validation = validation_data
  ))
}

# Function to create timing summary - reusable across experiments
create_timing_summary <- function(timing_data) {
  timing_data %>%
    group_by(run_name, n, t, nx) %>%
    summarise(
      mean_time_ms = mean(execution_time_ms),
      median_time_ms = median(execution_time_ms),
      sd_time_ms = sd(execution_time_ms),
      min_time_ms = min(execution_time_ms),
      max_time_ms = max(execution_time_ms),
      n_runs = n(),
      se_time_ms = sd_time_ms / sqrt(n_runs),
      .groups = "drop"
    ) %>%
    arrange(mean_time_ms)
}
```

```{r load_cumulative_data}
# Load cumulative gains study data
cumulative_experiment_data <- load_experiment_data(cumulative_experiment_name, results_dir)
cumulative_timing_data <- cumulative_experiment_data$timing
cumulative_estimates_data <- cumulative_experiment_data$estimates
cumulative_validation_data <- cumulative_experiment_data$validation
cumulative_timing_summary <- create_timing_summary(cumulative_timing_data)
```

```{r load_multithreading_data}
# Load multithreading study data
multithreading_experiment_data <- load_experiment_data(multithreading_experiment_name, results_dir)
multithreading_timing_data <- multithreading_experiment_data$timing
multithreading_estimates_data <- multithreading_experiment_data$estimates
multithreading_validation_data <- multithreading_experiment_data$validation
multithreading_timing_summary <- create_timing_summary(multithreading_timing_data)
```

```{r load_step_size_data}
# Load multithreading study data
step_size_experiment_data <- load_experiment_data(step_size_experiment_name, results_dir)
step_size_timing_data <- step_size_experiment_data$timing
step_size_estimates_data <- step_size_experiment_data$estimates
step_size_validation_data <- step_size_experiment_data$validation
```

```{r load_step_size_data}
# Load multithreading study data
step_size_experiment_fixed_data <- load_experiment_data(step_size_experiment_fixed_name, results_dir)
step_size_fixed_timing_data <- step_size_experiment_fixed_data$timing
step_size_fixed_estimates_data <- step_size_experiment_fixed_data$estimates
step_size_fixed_validation_data <- step_size_experiment_fixed_data$validation
```

```{r load_root_finding_data}
# Load multithreading study data
root_finding_experiment_data <- load_experiment_data(root_finding_experiment_name, results_dir)
root_finding_timing_data <- root_finding_experiment_data$timing  # FIXED: using correct data source
root_finding_estimates_data <- root_finding_experiment_data$estimates
root_finding_validation_data <- root_finding_experiment_data$validation
```

# Cumulative Performance Analysis

```{r cumulative_data_prep}
# Prepare cumulative optimization data
cumulative_data <- cumulative_timing_summary %>%
  # filter(str_detect(run_name, "cpp_")) %>%
  filter(run_name != "cpp_split_aberth_parallel_thopt_o3-native-loop") %>%
  mutate(
    time_seconds = mean_time_ms / 1000,
    optimization = case_when(
      run_name == "r_original" ~ "Original R",
      run_name == "cpp_nothing_else" ~ "C++ Translation",
      run_name == "cpp_split" ~ "Matrix Splitting", 
      run_name == "cpp_split_aberth" ~ "Aberth-Ehrlich Root Finding",
      run_name == "cpp_split_aberth_parallel" ~ "Parallelization",
      run_name == "cpp_split_aberth_parallel_thopt" ~ "Model Caching",
      run_name == "cpp_split_aberth_parallel_thopt_o3-native-loop_flto" ~ "Compiler Optimizations"
    ),
    optimization = factor(optimization, levels = c(
      "Original R","C++ Translation", "Matrix Splitting", "Aberth-Ehrlich Root Finding", 
      "Parallelization", "Model Caching", "Compiler Optimizations"
    ))
  ) %>%
  arrange(desc(time_seconds))
```

```{r cumulative_plot}


# Create cumulative performance plot with points and log scale
ggplot(cumulative_data, aes(x = optimization, y = time_seconds)) +
  geom_point(size = 5, color = "#2980b9", alpha = 0.8) +
  geom_segment(aes(xend = optimization, yend = 0.5), 
               color = "#2980b9", alpha = 0.6, size = 1.2) +
  geom_text(aes(label = sprintf("%.2f s", time_seconds)),
            vjust = -1.2, size = 4.2, color = "#2c3e50") +
  scale_y_log10(
    breaks = c(0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500),
    labels = c("0.5", "1", "2", "5", "10", "20", "50", "100", "200", "500"),
    limits = c(0.5, 500),
    expand = c(0.02, 0)
  ) +
  annotation_logticks(sides = "l", size = 0.4) +
  labs(
    title = "Cumulative Performance Improvements",
    subtitle = "BISAM Framework Optimization (n=5, t=15) - Log Scale",
    x = NULL,
    y = "Execution Time (seconds) - Log Scale"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "grey90", size = 0.4),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    axis.title.y = element_text(size = 13, margin = margin(r = 15)),
    plot.title = element_text(size = 16, face = "bold", margin = margin(b = 8)),
    plot.subtitle = element_text(size = 13, color = "#666666", margin = margin(b = 25)),
    plot.margin = margin(5, 0, 10, 5)
  )

ggsave("~/Documents/bisam-thesis/graphics/cumulative.png",
       width = 10, height = 6.5, dpi = 300, units = "in")
```

```{r cumulative_analysis}
# Calculate speedup factors and create table
baseline_time <- cumulative_data$time_seconds[cumulative_data$optimization == "C++ Translation"]
final_time <- cumulative_data$time_seconds[cumulative_data$optimization == "Compiler Optimizations"]
cumulative_speedup <- baseline_time / final_time
cat(sprintf("Overall speedup: %.1fx faster (from %.2f to %.2f seconds)\n",
            cumulative_speedup, baseline_time, final_time))

# Create speedup table with step-by-step and total speedups
cumulative_speedup_data <- cumulative_data %>%
  arrange(desc(time_seconds)) %>%  # Arrange from slowest to fastest
  mutate(
    baseline_time = first(time_seconds),
    previous_time = lag(time_seconds, default = first(time_seconds)),
    step_speedup = ifelse(row_number() == 1, 1.0, previous_time / time_seconds),
    total_speedup = baseline_time / time_seconds
  ) %>%
  select(optimization, time_seconds, step_speedup, total_speedup)

# Generate formatted data for LaTeX table
formatted_data <- cumulative_speedup_data %>%
  mutate(
    time_formatted = sprintf("%.2f", time_seconds),
    step_speedup_formatted = sprintf("%.1f", step_speedup),
    total_speedup_formatted = sprintf("%.1f", total_speedup)
  )

# Print the data to verify
print(formatted_data)

# Generate LaTeX table in the exact format you want
cat("\\begin{table}[h!]\n")
cat("\\centering\n")
cat("\\caption{Performance improvements from cumulative optimizations}\n")
cat("\\label{tab:performance_improvements_clean}\n")
cat("\\begin{tabular}{l S[table-format=3.2] S[table-format=2.1] S[table-format=2.1]}\n")
cat("\\toprule\n")
cat("Optimization Step & {Time (s)} & {Step Speedup} & {Total Speedup} \\\\\n")
cat("\\midrule\n")

# Generate table rows
for(i in 1:nrow(formatted_data)) {
  cat(sprintf("%s & %s & %s & %s \\\\\n",
              formatted_data$optimization[i],
              formatted_data$time_formatted[i],
              formatted_data$step_speedup_formatted[i],
              formatted_data$total_speedup_formatted[i]))
}

cat("\\bottomrule\n")
cat("\\end{tabular}\n")
cat("\\end{table}\n")
```

# Root Finder Comparison Speed

```{r}
# Prepare root finding comparison data
root_finding_comparison <- root_finding_timing_data %>%
  group_by(run_name) %>%
  summarise(
    mean_time_ms = mean(execution_time_ms),
    median_time_ms = median(execution_time_ms),
    sd_time_ms = sd(execution_time_ms),
    min_time_ms = min(execution_time_ms),
    max_time_ms = max(execution_time_ms),
    n_runs = n(),
    se_time_ms = sd_time_ms / sqrt(n_runs),
    .groups = "drop"
  ) %>%
  mutate(
    time_seconds = mean_time_ms / 1000,
    algorithm = case_when(
      run_name == "JenkinsTraub" ~ "Jenkins-Traub",
      run_name == "Newton" ~ "Newton-Raphson",
      run_name == "Aberth" ~ "Aberth-Ehrlich"
    ),
    algorithm = factor(algorithm, levels = c("Jenkins-Traub", "Newton-Raphson", "Aberth-Ehrlich"))
  ) %>%
  arrange(desc(time_seconds))

# Create root finding comparison plot with consistent styling
ggplot(root_finding_comparison, aes(x = algorithm, y = time_seconds, fill = algorithm)) +
  geom_col(alpha = 0.8, width = 0.7) +
  geom_errorbar(aes(ymin = time_seconds - se_time_ms/1000, 
                    ymax = time_seconds + se_time_ms/1000),
                width = 0.2, color = "#2c3e50", alpha = 0.7) +
  geom_text(aes(label = sprintf("%.2f s", time_seconds)),
            vjust = -0.8, size = 4.2, color = "#2c3e50") +
  scale_fill_manual(values = c("Jenkins-Traub" = "#2c3e50", 
                                "Newton-Raphson" = "#e74c3c", 
                                "Aberth-Ehrlich" = "#3498db")) +
  scale_y_continuous(
    breaks = seq(0, 35, 5),
    limits = c(0, max(root_finding_comparison$time_seconds) * 1.15),
    expand = c(0, 0)
  ) +
  labs(
    title = "Root Finding Algorithm Performance Comparison",
    subtitle = "BISAM Framework (n=10, t=20)",
    x = NULL,
    y = "Execution Time (seconds)"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "grey90", size = 0.4),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12),
    axis.title.y = element_text(size = 13, margin = margin(r = 15)),
    plot.title = element_text(size = 16, face = "bold", margin = margin(b = 8)),
    plot.subtitle = element_text(size = 13, color = "#666666", margin = margin(b = 25)),
    plot.margin = margin(5, 0, 10, 5),
    legend.position = "none"  # Hide legend since algorithm names are on x-axis
  )

ggsave("~/Documents/bisam-thesis/graphics/root_finding_comparison.png",
       width = 10, height = 6.5, dpi = 300, units = "in")

# Print summary statistics
cat("\nRoot Finding Algorithm Performance Summary:\n")
cat("==========================================\n")
root_finding_comparison %>%
  mutate(
    speedup_vs_jenkins = max(time_seconds) / time_seconds
  ) %>%
  select(algorithm, time_seconds, speedup_vs_jenkins, n_runs) %>%
  kable(digits = 2, col.names = c("Algorithm", "Mean Time (s)", 
                                   "Speedup vs Jenkins-Traub", "N Runs"),
        format = "markdown")
```

# Multithreading Performance Analysis

```{r multithreading_data_prep}
# Prepare multithreading data
threading_data <- multithreading_timing_summary %>%
  filter(str_detect(run_name, "\\d+-core")) %>%
  mutate(
    core_count = as.numeric(str_extract(run_name, "\\d+(?=-core)")),
    time_seconds = mean_time_ms / 1000,
    time_se_seconds = se_time_ms / 1000,
    ci_lower = time_seconds - 1.96 * time_se_seconds,
    ci_upper = time_seconds + 1.96 * time_se_seconds
  ) %>%
  arrange(core_count)
```

```{r multithreading_plot}
# Create multithreading performance plot
ggplot(threading_data, aes(x = core_count, y = time_seconds)) +
  geom_line(color = "#2980b9", size = 1.2) +
  geom_point(color = "#2980b9", size = 3, alpha = 0.8) +
  scale_x_continuous(
    breaks = threading_data$core_count,
    labels = threading_data$core_count
  ) +
  scale_y_continuous(
    limits = c(0, max(threading_data$time_seconds) * 1.05),
    expand = c(0, 0)
  ) +
  labs(
    title = "Multithreading Performance Scaling",
    subtitle = sprintf("Runtime vs Core Count (n=%d, t=%d)", 
                      threading_data$n[1], threading_data$t[1]),
    x = "Number of Cores",
    y = "Execution Time (seconds)"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "#f8f9fa", size = 0.5),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11),
    axis.title.x = element_text(size = 11, margin = margin(t = 15)),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20)),
    plot.margin = margin(5, 0, 10, 5)
  )

  ggsave("~/Documents/bisam-thesis/graphics/parallelization.png",
       width = 9, height = 5.5, dpi = 300, units = "in")
```

```{r multithreading_analysis}
# Calculate threading efficiency
threading_baseline_time <- threading_data$time_seconds[threading_data$core_count == min(threading_data$core_count)]

threading_efficiency_data <- threading_data %>%
  mutate(
    speedup = threading_baseline_time / time_seconds,
    efficiency = speedup / core_count * 100,
    theoretical_speedup = core_count,
    parallel_efficiency = speedup / theoretical_speedup * 100
  ) %>%
  select(core_count, time_seconds, speedup, efficiency, parallel_efficiency)

cat("Multithreading Performance Summary:\n")
cat("=====================================\n")

threading_efficiency_table <- threading_efficiency_data %>%
  mutate(
    time_formatted = paste0(round(time_seconds, 3), "s"),
    speedup_formatted = paste0(round(speedup, 2), "×"),
    efficiency_formatted = paste0(round(parallel_efficiency, 1), "%")
  ) %>%
  select(
    "Cores" = core_count,
    "Runtime" = time_formatted,
    "Speedup" = speedup_formatted,
    "Parallel Efficiency" = efficiency_formatted
  )

print(kable(threading_efficiency_table, format = "html",
      caption = "Multithreading scaling efficiency"))

# Summary statistics
threading_max_speedup <- max(threading_efficiency_data$speedup)
threading_best_efficiency <- max(threading_efficiency_data$parallel_efficiency)

cat(sprintf("\nMaximum speedup achieved: %.2fx with %d cores\n", 
            threading_max_speedup, threading_efficiency_data$core_count[which.max(threading_efficiency_data$speedup)]))
cat(sprintf("Best parallel efficiency: %.1f%% with %d cores\n", 
            threading_best_efficiency, threading_efficiency_data$core_count[which.max(threading_efficiency_data$parallel_efficiency)]))
```

# Step Size Analysis

```{r}

# Extract algorithm and step size from run_name
step_size_estimates_clean <- step_size_fixed_estimates_data %>%
  filter(run_name != "experiment_name") %>%  # Remove header row if present
  mutate(
    algorithm = str_extract(run_name, "^[a-z]+"),
    step_size = as.numeric(str_extract(run_name, "\\d+$")) / 100,
    parameter_index = as.numeric(parameter_index),
    true_value = as.numeric(true_value),
    estimated_value = as.numeric(estimated_value),
    error = as.numeric(error),
    probability = as.numeric(probability)
  ) %>%
  filter(!is.na(algorithm), !is.na(step_size))

step_size_validation_clean <- step_size_validation_data %>%
  filter(run_name != "experiment_name") %>%
  mutate(
    algorithm = str_extract(run_name, "^[a-z]+"),
    step_size = as.numeric(str_extract(run_name, "\\d+$")) / 100,
    beta_mse = as.numeric(beta_mse),
    beta_max_error = as.numeric(beta_max_error),
    break_detected_correctly = as.logical(break_detected_correctly),
    true_break_position = as.numeric(true_break_position),
    detected_break_position = as.numeric(detected_break_position),
    detected_break_probability = as.numeric(detected_break_probability)
  ) %>%
  filter(!is.na(algorithm), !is.na(step_size))

cat("Step size data loaded:\n")
cat("  Algorithms:", paste(unique(step_size_estimates_clean$algorithm), collapse = ", "), "\n")
cat("  Step sizes:", paste(sort(unique(step_size_estimates_clean$step_size)), collapse = ", "), "\n")
cat("  Datasets:", length(unique(step_size_estimates_clean$dataset_name)), "\n")

```

```{r}
# Analyze beta coefficient accuracy across algorithms
beta_accuracy <- step_size_estimates_clean %>%
  filter(parameter_type == "beta") %>%
  group_by(algorithm, step_size, dataset_name, run_number) %>%
  summarise(
    beta_mse = mean(error^2, na.rm = TRUE),
    beta_max_error = max(abs(error), na.rm = TRUE),
    beta_mean_abs_error = mean(abs(error), na.rm = TRUE),
    n_betas = n(),
    .groups = "drop"
  ) %>%
  group_by(algorithm, step_size) %>%
  summarise(
    mean_mse = mean(beta_mse, na.rm = TRUE),
    mean_max_error = mean(beta_max_error, na.rm = TRUE),
    mean_abs_error = mean(beta_mean_abs_error, na.rm = TRUE),
    sd_mse = sd(beta_mse, na.rm = TRUE),
    sd_max_error = sd(beta_max_error, na.rm = TRUE),
    sd_abs_error = sd(beta_mean_abs_error, na.rm = TRUE),  # Added this line
    n_runs = n(),
    .groups = "drop"
  )

# Create comparison against Jenkins baseline
jenkins_baseline <- beta_accuracy %>%
  filter(algorithm == "jenkins") %>%
  select(step_size, jenkins_mse = mean_mse, jenkins_max_error = mean_max_error, jenkins_abs_error = mean_abs_error)

beta_comparison <- beta_accuracy %>%
  left_join(jenkins_baseline, by = "step_size") %>%
  mutate(
    mse_ratio = mean_mse / jenkins_mse,
    max_error_ratio = mean_max_error / jenkins_max_error,
    abs_error_ratio = mean_abs_error / jenkins_abs_error,
    mse_diff = mean_mse - jenkins_mse,
    algorithm_factor = factor(algorithm, levels = c("jenkins", "newton", "aberth"))
  )

print(kable(beta_comparison %>% 
              select(algorithm, step_size, mean_mse, mse_ratio, mean_max_error, max_error_ratio) %>%
              mutate(across(where(is.numeric), ~round(.x, 4))),
            format = "markdown", caption = "Beta estimation accuracy comparison"))
```

```{r}
# Plot beta estimation accuracy across algorithms and step sizes
ggplot(beta_comparison, aes(x = factor(step_size), y = mean_abs_error, color = algorithm_factor, fill = algorithm_factor)) +
  geom_ribbon(aes(ymin = mean_abs_error - sd_abs_error, 
                  ymax = mean_abs_error + sd_abs_error,
                  group = algorithm_factor), 
              alpha = 0.1, color = NA) +
  geom_line(size = 1.2, alpha = 0.8, aes(group = algorithm_factor)) +
  geom_point(size = 3, alpha = 0.9) +
  scale_color_manual(
    values = c("jenkins" = "#2c3e50", "newton" = "#e74c3c", "aberth" = "#3498db"),
    labels = c("Jenkins (Baseline)", "Newton-Raphson", "Aberth-Ehrlich")
  ) +
  scale_fill_manual(
    values = c("jenkins" = "#2c3e50", "newton" = "#e74c3c", "aberth" = "#3498db"),
    labels = c("Jenkins (Baseline)", "Newton-Raphson", "Aberth-Ehrlich"),
    guide = "none"  # Hide fill legend since it's the same as color
  ) +
  scale_x_discrete(
    labels = c("0.5", "0.75", "1", "1.5", "3", "5")
  ) +
  scale_y_continuous(
    limits = c(min(beta_comparison$mean_abs_error) * 0.8, max(beta_comparison$mean_abs_error) * 1.1),
    expand = c(0, 0)
  ) +
  labs(
    title = "Beta Coefficient Estimation Accuracy",
    subtitle = "Mean Absolute Error by Root Finding Algorithm and Step Size",
    x = "Step Size",
    y = "Mean Absolute Error",
    color = "Algorithm"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "#f8f9fa", size = 0.5),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    axis.title.x = element_text(size = 11, margin = margin(t = 15)),
    axis.title.y = element_text(size = 11, margin = margin(r = 15)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20)),
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9),
    plot.margin = margin(5, 0, 10, 5)
  )

ggsave("~/Documents/bisam-thesis/graphics/step-size-beta.png",
       width = 9, height = 5.5, dpi = 300, units = "in")
```

```{r}
# Analyze break detection accuracy
break_detection_summary <- step_size_estimates_clean %>%
  filter(parameter_type == "break_indicator") %>%
  group_by(algorithm, step_size, dataset_name, run_number) %>%
  summarise(
    n_indicators = n(),
    mean_probability = mean(probability, na.rm = TRUE),
    max_probability = max(probability, na.rm = TRUE),
    n_high_prob = sum(probability > 0.5, na.rm = TRUE),
    detected_break_position = which.max(probability),
    detected_break_probability = max(probability, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(algorithm, step_size) %>%
  summarise(
    mean_detection_prob = mean(detected_break_probability, na.rm = TRUE),
    sd_detection_prob = sd(detected_break_probability, na.rm = TRUE),
    mean_position = mean(detected_break_position, na.rm = TRUE),
    sd_position = sd(detected_break_position, na.rm = TRUE),
    n_runs = n(),
    .groups = "drop"
  )

# Compare break detection against Jenkins
jenkins_break_baseline <- break_detection_summary %>%
  filter(algorithm == "jenkins") %>%
  select(step_size, jenkins_prob = mean_detection_prob, jenkins_position = mean_position)

break_comparison <- break_detection_summary %>%
  left_join(jenkins_break_baseline, by = "step_size") %>%
  mutate(
    prob_diff = mean_detection_prob - jenkins_prob,
    position_diff = mean_position - jenkins_position,
    algorithm_factor = factor(algorithm, levels = c("jenkins", "newton", "aberth"))
  )

print(kable(break_comparison %>%
              select(algorithm, step_size, mean_detection_prob, prob_diff, mean_position, position_diff) %>%
              mutate(across(where(is.numeric), ~round(.x, 4))),
            format = "markdown", caption = "Break detection comparison"))
```

```{r}
# Plot break detection probability
ggplot(break_comparison, aes(x = step_size, y = mean_detection_prob, color = algorithm_factor)) +
  geom_line(size = 1.2, alpha = 0.8) +
  geom_point(size = 3, alpha = 0.9) +
  geom_ribbon(aes(ymin = mean_detection_prob - sd_detection_prob, 
                  ymax = mean_detection_prob + sd_detection_prob,
                  fill = algorithm_factor), 
              alpha = 0.2, color = NA) +
  scale_color_manual(
    values = c("jenkins" = "#2c3e50", "newton" = "#e74c3c", "aberth" = "#3498db"),
    labels = c("Jenkins (Baseline)", "Newton-Raphson", "Aberth-Ehrlich")
  ) +
  scale_fill_manual(
    values = c("jenkins" = "#2c3e50", "newton" = "#e74c3c", "aberth" = "#3498db"),
    guide = "none"
  ) +
  scale_x_continuous(
    breaks = unique(break_comparison$step_size),
    labels = paste0(unique(break_comparison$step_size) * 100, "%")
  ) +
  scale_y_continuous(
    limits = c(0.39, 1.0),
    labels = scales::percent_format(accuracy = 0.1)
  ) +
  labs(
    title = "Break Detection Probability",
    subtitle = "Posterior Inclusion Probability by Root Finding Algorithm and Step Size",
    x = "Step Size", 
    y = "Detection Probability",
    color = "Algorithm"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "#f8f9fa", size = 0.5),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20)),
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9),
    plot.margin = margin(20, 20, 20, 20)
  )
```

```{r}
# Calculate detailed differences from Jenkins baseline
detailed_comparison <- step_size_estimates_clean %>%
  filter(parameter_type %in% c("beta", "break_indicator")) %>%
  select(algorithm, step_size, dataset_name, run_number, parameter_type, 
         parameter_index, estimated_value, probability) %>%
  pivot_wider(
    names_from = algorithm, 
    values_from = c(estimated_value, probability),
    names_sep = "_",
    values_fn = mean  # This will average any duplicate values
  ) %>%
  mutate(
    newton_estimate_diff = estimated_value_newton - estimated_value_jenkins,
    aberth_estimate_diff = estimated_value_aberth - estimated_value_jenkins,
    newton_prob_diff = probability_newton - probability_jenkins,
    aberth_prob_diff = probability_aberth - probability_jenkins
  )

# Summarize differences by parameter type and step size
difference_summary <- detailed_comparison %>%
  group_by(parameter_type, step_size) %>%
  summarise(
    newton_mean_estimate_diff = mean(abs(newton_estimate_diff), na.rm = TRUE),
    aberth_mean_estimate_diff = mean(abs(aberth_estimate_diff), na.rm = TRUE),
    newton_max_estimate_diff = max(abs(newton_estimate_diff), na.rm = TRUE),
    aberth_max_estimate_diff = max(abs(aberth_estimate_diff), na.rm = TRUE),
    newton_mean_prob_diff = mean(abs(newton_prob_diff), na.rm = TRUE),
    aberth_mean_prob_diff = mean(abs(aberth_prob_diff), na.rm = TRUE),
    n_comparisons = n(),
    .groups = "drop"
  )

# print(kable(difference_summary %>%
#               mutate(across(where(is.numeric), ~round(.x, 6))),
#             format = "markdown", caption = "Absolute differences from Jenkins baseline"))
```

```{r}
# Plot error distributions for each algorithm
error_distributions <- step_size_estimates_clean %>%
  filter(parameter_type == "beta") %>%
  mutate(
    algorithm_factor = factor(algorithm, levels = c("jenkins", "newton", "aberth")),
    step_size_factor = factor(paste0(step_size * 100, "%"))
  )

ggplot(error_distributions, aes(x = abs(error), fill = algorithm_factor)) +
  geom_density(alpha = 0.7, adjust = 1.2) +
  facet_wrap(~step_size_factor, scales = "free_y", labeller = label_both) +
  scale_fill_manual(
    values = c("jenkins" = "#2c3e50", "newton" = "#e74c3c", "aberth" = "#3498db"),
    labels = c("Jenkins (Baseline)", "Newton-Raphson", "Aberth-Ehrlich")
  ) +
  scale_x_continuous(
    trans = "log10",
    labels = scales::trans_format("log10", scales::math_format(10^.x))
  ) +
  labs(
    title = "Beta Estimation Error Distributions",
    subtitle = "Absolute Error Density by Algorithm and Step Size (Log Scale)",
    x = "Absolute Error (Log Scale)",
    y = "Density",
    fill = "Algorithm"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "#f8f9fa", size = 0.3),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 9),
    axis.title = element_text(size = 10),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20)),
    legend.position = "bottom",
    legend.title = element_text(size = 9),
    legend.text = element_text(size = 8),
    strip.text = element_text(size = 9),
    plot.margin = margin(20, 20, 20, 20)
  )
```

```{r}
# Analyze consistency across step sizes for each algorithm
consistency_analysis <- step_size_estimates_clean %>%
  filter(parameter_type == "beta") %>%
  group_by(algorithm, dataset_name, run_number, parameter_index) %>%
  summarise(
    error_variance = var(error, na.rm = TRUE),
    error_range = max(error, na.rm = TRUE) - min(error, na.rm = TRUE),
    mean_abs_error = mean(abs(error), na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(algorithm) %>%
  summarise(
    mean_error_variance = mean(error_variance, na.rm = TRUE),
    mean_error_range = mean(error_range, na.rm = TRUE),
    overall_consistency = 1 / (1 + mean_error_variance),  # Higher is more consistent
    .groups = "drop"
  ) %>%
  arrange(desc(overall_consistency))

print(kable(consistency_analysis %>%
              mutate(across(where(is.numeric), ~round(.x, 6))),
            format = "markdown", caption = "Algorithm consistency across step sizes"))
```

```{r}
# Create final performance summary
cat("\n=== ROOT FINDING ALGORITHM COMPARISON SUMMARY ===\n")
cat("=================================================\n\n")

# Beta estimation summary
beta_summary <- beta_comparison %>%
  group_by(algorithm) %>%
  summarise(
    avg_mse_ratio = mean(mse_ratio, na.rm = TRUE),
    avg_max_error_ratio = mean(max_error_ratio, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(avg_mse_ratio)

cat("Beta Estimation Performance (relative to Jenkins):\n")
for(i in 1:nrow(beta_summary)) {
  alg <- beta_summary$algorithm[i]
  mse_ratio <- round(beta_summary$avg_mse_ratio[i], 3)
  max_error_ratio <- round(beta_summary$avg_max_error_ratio[i], 3)
  
  if(alg == "jenkins") {
    cat(sprintf("  %s: Baseline (MSE=1.000, Max Error=1.000)\n", str_to_title(alg)))
  } else {
    performance <- ifelse(mse_ratio < 1.01, "≈ Same", ifelse(mse_ratio < 1.05, "Slightly worse", "Notably worse"))
    cat(sprintf("  %s: %s (MSE=%.3f, Max Error=%.3f)\n", str_to_title(alg), performance, mse_ratio, max_error_ratio))
  }
}

# Break detection summary  
break_summary <- break_comparison %>%
  group_by(algorithm) %>%
  summarise(
    avg_prob_diff = mean(abs(prob_diff), na.rm = TRUE),
    avg_position_diff = mean(abs(position_diff), na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(avg_prob_diff)

cat("\nBreak Detection Performance (absolute difference from Jenkins):\n")
for(i in 1:nrow(break_summary)) {
  alg <- break_summary$algorithm[i]
  prob_diff <- round(break_summary$avg_prob_diff[i], 4)
  pos_diff <- round(break_summary$avg_position_diff[i], 2)
  
  if(alg == "jenkins") {
    cat(sprintf("  %s: Baseline (Prob Diff=0.0000, Position Diff=0.00)\n", str_to_title(alg)))
  } else {
    performance <- ifelse(prob_diff < 0.001, "≈ Same", ifelse(prob_diff < 0.01, "Slightly different", "Notably different"))
    cat(sprintf("  %s: %s (Prob Diff=%.4f, Position Diff=%.2f)\n", str_to_title(alg), performance, prob_diff, pos_diff))
  }
}

# Overall recommendation
cat("\nRECOMMENDATION:\n")
newton_performance <- beta_summary$avg_mse_ratio[beta_summary$algorithm == "newton"]
aberth_performance <- beta_summary$avg_mse_ratio[beta_summary$algorithm == "aberth"]

if(min(newton_performance, aberth_performance, na.rm = TRUE) < 1.02) {
  best_alg <- ifelse(newton_performance < aberth_performance, "Newton-Raphson", "Aberth-Ehrlich")
  cat(sprintf("Both alternative algorithms produce nearly identical results to Jenkins.\n"))
  cat(sprintf("%s shows marginally better performance and could be used as a replacement.\n", best_alg))
} else {
  cat("Jenkins remains the most accurate algorithm. Alternative algorithms show measurable differences.\n")
}

cat("\n=================================================\n")
```

# OPUS

```{r}
# Parse run names to extract algorithm and step size
step_size_estimates_data <- step_size_estimates_data %>%
  mutate(
    algorithm = str_extract(run_name, "^[^_]+"),
    step_size_str = str_extract(run_name, "\\d+$"),
    step_size = as.numeric(step_size_str) / 100
  )

# Create a function to determine if a break was detected (using 0.5 threshold)
detection_threshold <- 0.5

# Separate beta estimates from break indicators
beta_estimates <- step_size_estimates_data %>%
  filter(parameter_type == "beta") %>%
  select(algorithm, step_size, dataset_name, run_number, parameter_index, 
         true_value, estimated_value, error)

break_indicators <- step_size_estimates_data %>%
  filter(parameter_type == "break_indicator") %>%
  mutate(detected = probability > detection_threshold) %>%
  select(algorithm, step_size, dataset_name, run_number, parameter_index, 
         probability, detected)

sigma_estimates <- step_size_estimates_data %>%
  filter(parameter_type == "sigma2") %>%
  select(algorithm, step_size, dataset_name, run_number, 
         true_value, estimated_value, error)
```

```{r}
# Compare beta estimates across algorithms
beta_comparison <- beta_estimates %>%
  group_by(algorithm, step_size, parameter_index) %>%
  summarise(
    mean_error = mean(abs(error)),
    sd_error = sd(abs(error)),
    mse = mean(error^2),
    max_error = max(abs(error)),
    n_runs = n(),
    .groups = "drop"
  )

# Create beta error comparison plot
ggplot(beta_comparison, aes(x = factor(step_size), y = mse, color = algorithm)) +
  geom_point(size = 3, alpha = 0.8, position = position_dodge(width = 0.3)) +
  geom_line(aes(group = interaction(algorithm, parameter_index)), 
            alpha = 0.4, position = position_dodge(width = 0.3)) +
  facet_wrap(~paste("Beta", parameter_index), scales = "free_y") +
  scale_color_manual(values = c("jenkins" = "#2980b9", 
                                "newton" = "#e74c3c", 
                                "aberth" = "#27ae60")) +
  labs(
    title = "Beta Estimation Error Across Root Finding Algorithms",
    subtitle = "Mean Squared Error by Step Size",
    x = "Step Size",
    y = "Mean Squared Error",
    color = "Algorithm"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20))
  )
```

```{r}
# Analyze break detection accuracy
# First, identify which breaks are true breaks (from the data generation)
# Since we don't have the true break positions explicitly, we'll use Jenkins as reference

jenkins_breaks <- break_indicators %>%
  filter(algorithm == "jenkins") %>%
  group_by(step_size, dataset_name, parameter_index) %>%
  summarise(
    jenkins_prob = mean(probability),
    jenkins_detected = mean(detected),
    .groups = "drop"
  )

# Compare other algorithms to Jenkins
break_comparison <- break_indicators %>%
  group_by(algorithm, step_size, dataset_name, parameter_index) %>%
  summarise(
    mean_prob = mean(probability),
    detection_rate = mean(detected),
    .groups = "drop"
  ) %>%
  left_join(jenkins_breaks, by = c("step_size", "dataset_name", "parameter_index")) %>%
  mutate(
    prob_diff_from_jenkins = mean_prob - jenkins_prob,
    detection_agreement = (detection_rate == jenkins_detected)
  )

# Find the true break location (highest probability in Jenkins)
true_breaks <- jenkins_breaks %>%
  group_by(step_size, dataset_name) %>%
  filter(jenkins_prob == max(jenkins_prob)) %>%
  mutate(is_true_break = TRUE) %>%
  select(step_size, dataset_name, parameter_index, is_true_break)

# Calculate detection performance for true breaks
break_detection_performance <- break_comparison %>%
  left_join(true_breaks, by = c("step_size", "dataset_name", "parameter_index")) %>%
  mutate(is_true_break = ifelse(is.na(is_true_break), FALSE, is_true_break)) %>%
  group_by(algorithm, step_size, is_true_break) %>%
  summarise(
    mean_probability = mean(mean_prob),
    sd_probability = sd(mean_prob),
    detection_rate = mean(detection_rate),
    .groups = "drop"
  )

# Plot break detection performance
ggplot(break_detection_performance %>% filter(is_true_break), 
       aes(x = factor(step_size), y = mean_probability, fill = algorithm)) +
  geom_col(position = "dodge", alpha = 0.8, width = 0.7) +
  geom_hline(yintercept = detection_threshold, linetype = "dashed", 
             color = "#7f8c8d", alpha = 0.7) +
  annotate("text", x = Inf, y = detection_threshold + 0.02, 
           label = "Detection Threshold", hjust = 1.1, 
           color = "#7f8c8d", size = 3) +
  scale_fill_manual(values = c("jenkins" = "#2980b9", 
                               "newton" = "#e74c3c", 
                               "aberth" = "#27ae60")) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(
    title = "Break Detection Probability at True Break Locations",
    subtitle = "Posterior Inclusion Probability by Algorithm and Step Size",
    x = "Step Size",
    y = "Mean Posterior Probability",
    fill = "Algorithm"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid.major.x = element_blank(),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20))
  )
```

```{r}
# Calculate agreement between algorithms
algorithm_pairs <- list(
  c("jenkins", "newton"),
  c("jenkins", "aberth"),
  c("newton", "aberth")
)

agreement_data <- data.frame()

for (pair in algorithm_pairs) {
  alg1_data <- break_indicators %>%
    filter(algorithm == pair[1]) %>%
    select(step_size, dataset_name, run_number, parameter_index, 
           prob1 = probability, detected1 = detected)
  
  alg2_data <- break_indicators %>%
    filter(algorithm == pair[2]) %>%
    select(step_size, dataset_name, run_number, parameter_index, 
           prob2 = probability, detected2 = detected)
  
  pair_agreement <- alg1_data %>%
    inner_join(alg2_data, by = c("step_size", "dataset_name", "run_number", "parameter_index")) %>%
    mutate(
      prob_diff = abs(prob1 - prob2),
      detection_match = (detected1 == detected2),
      pair_name = paste(pair[1], "vs", pair[2])
    )
  
  agreement_data <- bind_rows(agreement_data, pair_agreement)
}

# Summarize agreement
agreement_summary <- agreement_data %>%
  group_by(pair_name, step_size) %>%
  summarise(
    mean_prob_diff = mean(prob_diff),
    sd_prob_diff = sd(prob_diff),
    max_prob_diff = max(prob_diff),
    detection_agreement_rate = mean(detection_match),
    .groups = "drop"
  )

# Plot probability differences
ggplot(agreement_summary, aes(x = factor(step_size), y = mean_prob_diff, 
                              color = pair_name, group = pair_name)) +
  geom_line(size = 1.2, alpha = 0.8) +
  geom_point(size = 3, alpha = 0.8) +
  scale_color_manual(values = c(
    "jenkins vs newton" = "#8e44ad",
    "jenkins vs aberth" = "#d35400",
    "newton vs aberth" = "#16a085"
  )) +
  labs(
    title = "Algorithm Agreement on Break Probabilities",
    subtitle = "Mean Absolute Difference in Posterior Probabilities",
    x = "Step Size",
    y = "Mean Absolute Probability Difference",
    color = "Algorithm Pair"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20))
  )
```

```{r}
# Analyze break localization accuracy (fuzziness around true break)
# For each true break, look at probability distribution around it

# Get probability profiles around true breaks
break_profiles <- break_indicators %>%
  left_join(true_breaks, by = c("step_size", "dataset_name", "parameter_index")) %>%
  group_by(algorithm, step_size, dataset_name) %>%
  mutate(
    max_prob_idx = parameter_index[which.max(probability)],
    true_break_idx = parameter_index[which(is_true_break == TRUE)[1]]
  ) %>%
  filter(!is.na(true_break_idx)) %>%
  mutate(
    distance_from_true = abs(parameter_index - true_break_idx),
    relative_position = parameter_index - true_break_idx
  ) %>%
  group_by(algorithm, step_size, distance_from_true) %>%
  summarise(
    mean_probability = mean(probability),
    se_probability = sd(probability) / sqrt(n()),
    .groups = "drop"
  )

# Plot probability decay around true break
ggplot(break_profiles %>% filter(distance_from_true <= 5), 
       aes(x = distance_from_true, y = mean_probability, color = algorithm)) +
  geom_line(size = 1.2, alpha = 0.8) +
  geom_point(size = 2.5, alpha = 0.8) +
  facet_wrap(~paste("Step Size:", step_size), scales = "free_y", nrow = 2) +
  scale_color_manual(values = c("jenkins" = "#2980b9", 
                                "newton" = "#e74c3c", 
                                "aberth" = "#27ae60")) +
  labs(
    title = "Break Localization Accuracy",
    subtitle = "Probability Distribution Around True Break Location",
    x = "Distance from True Break (time periods)",
    y = "Mean Posterior Probability",
    color = "Algorithm"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
    plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 20))
  )
```

# GPT

# Original Data

```{r}
# Function to load original simulation datasets
load_original_dataset <- function(step_size, results_dir = "./r-testing-files/simulation_data") {
  # Convert step_size to the format used in filenames
  step_size_str <- sprintf("%03d", step_size * 100)
  dataset_name <- paste0("rootfind_stepsize_", step_size_str)
  
  # Construct filename based on the pattern from data generation
  filename <- file.path(results_dir, paste0("sim_", dataset_name, "_n10_t30_nx03.rds"))
  
  if (file.exists(filename)) {
    return(readRDS(filename))
  } else {
    cat("File not found:", filename, "\n")
    return(NULL)
  }
}

# Load all step size datasets
step_sizes <- unique(step_size_estimates_clean$step_size)
original_datasets <- list()

for (size in step_sizes) {
  cat("Loading step size", size, "dataset...\n")
  original_datasets[[paste0("step_", size)]] <- load_original_dataset(size)
}

cat("Original datasets loaded for step sizes:", paste(step_sizes, collapse = ", "), "\n")
```

```{r}
# Extract detected breaks for each algorithm at each step size
detected_breaks <- step_size_estimates_clean %>%
  filter(parameter_type == "break_indicator") %>%
  group_by(algorithm, step_size, dataset_name, run_number) %>%
  arrange(parameter_index) %>%
  mutate(
    # Calculate which unit and time this break_indicator corresponds to
    # For n=10, t=30, we have (t-2) = 28 break indicators per unit
    # parameter_index 0-27 = unit 1, 28-55 = unit 2, etc.
    break_indicators_per_unit = 28,  # t-2 for t=30
    unit = floor(parameter_index / break_indicators_per_unit) + 1,
    time_within_unit = (parameter_index %% break_indicators_per_unit) + 2  # +2 because breaks start at time 2
  ) %>%
  filter(probability > 0.5) %>%  # Only consider high-probability detections
  ungroup()

# Get the most likely break for each algorithm/step_size/run combination
primary_detected_breaks <- step_size_estimates_clean %>%
  filter(parameter_type == "break_indicator") %>%
  group_by(algorithm, step_size, dataset_name, run_number) %>%
  arrange(desc(probability)) %>%
  slice(1) %>%
  mutate(
    break_indicators_per_unit = 28,
    unit = floor(parameter_index / break_indicators_per_unit) + 1,
    time_within_unit = (parameter_index %% break_indicators_per_unit) + 2
  ) %>%
  ungroup()

cat("Break detection data prepared\n")
```

```{r}
# Create time series plots for each step size
for (step_size in step_sizes) {
  cat("\nCreating plot for step size", step_size, "\n")
  
  # Get original dataset
  dataset_key <- paste0("step_", step_size)
  original_data <- original_datasets[[dataset_key]]
  
  if (is.null(original_data)) {
    cat("Skipping step size", step_size, "- data not available\n")
    next
  }
  
  # Extract time series data
  ts_data <- as.data.frame(original_data$sim_data$data) %>%
    mutate(
      unit = n,
      time = t,
      outcome = y
    ) %>%
    select(unit, time, outcome)
  
  # Get true break information
  true_breaks <- original_data$break_info %>%
    mutate(step_size_val = step_size)
  
  # Get detected breaks for this step size (average across runs)
  detected_for_this_size <- primary_detected_breaks %>%
    filter(step_size == !!step_size) %>%
    group_by(algorithm, unit, time_within_unit) %>%
    summarise(
      avg_probability = mean(probability, na.rm = TRUE),
      n_detections = n(),
      .groups = "drop"
    ) %>%
    filter(avg_probability > 0.3)  # Show if detected in multiple runs
  
  # Create the plot
  p <- ggplot(ts_data, aes(x = time, y = outcome)) +
    geom_line(aes(group = unit), alpha = 0.6, color = "#7f8c8d", size = 0.8) +
    facet_wrap(~unit, scales = "free_y", ncol = 5, 
               labeller = labeller(unit = function(x) paste("Unit", x))) +
    
    # Add true breaks as red vertical lines
    geom_vline(data = true_breaks, 
               aes(xintercept = time), 
               color = "#e74c3c", size = 1.2, alpha = 0.8) +
    
    # Add detected breaks for each algorithm
    {if (nrow(detected_for_this_size %>% filter(algorithm == "jenkins")) > 0)
      geom_vline(data = detected_for_this_size %>% filter(algorithm == "jenkins"),
                 aes(xintercept = time_within_unit), 
                 color = "#2c3e50", size = 1, alpha = 0.7, linetype = "dashed")
    } +
    
    {if (nrow(detected_for_this_size %>% filter(algorithm == "newton")) > 0)
      geom_vline(data = detected_for_this_size %>% filter(algorithm == "newton"),
                 aes(xintercept = time_within_unit), 
                 color = "#f39c12", size = 1, alpha = 0.7, linetype = "dotted")
    } +
    
    {if (nrow(detected_for_this_size %>% filter(algorithm == "aberth")) > 0)
      geom_vline(data = detected_for_this_size %>% filter(algorithm == "aberth"),
                 aes(xintercept = time_within_unit), 
                 color = "#3498db", size = 1, alpha = 0.7, linetype = "dotdash")
    } +
    
    scale_x_continuous(breaks = seq(0, 30, by = 5)) +
    labs(
      title = paste("Time Series with Break Detection - Step Size", paste0(step_size * 100, "%")),
      subtitle = paste("True breaks (red solid) vs Detected breaks:",
                      "Jenkins (dark dashed), Newton (orange dotted), Aberth (blue dot-dash)"),
      x = "Time Period",
      y = "Outcome Value"
    ) +
    theme_minimal() +
    theme(
      panel.grid.major = element_line(color = "#f8f9fa", size = 0.3),
      panel.grid.minor = element_blank(),
      axis.text = element_text(size = 8),
      axis.title = element_text(size = 10),
      plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
      plot.subtitle = element_text(size = 10, color = "#666666", margin = margin(b = 15)),
      strip.text = element_text(size = 9, face = "bold"),
      plot.margin = margin(20, 20, 20, 20)
    )
  
  # Print the plot
  print(p)
  
  # Print summary information
  cat("Step size", step_size, "summary:\n")
  cat("  True breaks:", nrow(true_breaks), "breaks at units", 
      paste(true_breaks$unit, collapse = ", "), "\n")
  cat("  Jenkins detections:", 
      nrow(detected_for_this_size %>% filter(algorithm == "jenkins")), "\n")
  cat("  Newton detections:", 
      nrow(detected_for_this_size %>% filter(algorithm == "newton")), "\n")
  cat("  Aberth detections:", 
      nrow(detected_for_this_size %>% filter(algorithm == "aberth")), "\n")
}
```

```{r}
# Create an aggregate view showing just the units with breaks
for (step_size in step_sizes) {
  cat("\nCreating aggregate plot for step size", step_size, "\n")
  
  dataset_key <- paste0("step_", step_size)
  original_data <- original_datasets[[dataset_key]]
  
  if (is.null(original_data)) next
  
  # Get units that have true breaks
  units_with_breaks <- original_data$break_info$unit
  
  # Filter time series data to only units with breaks
  ts_data_subset <- as.data.frame(original_data$sim_data$data) %>%
    mutate(unit = n, time = t, outcome = y) %>%
    filter(unit %in% units_with_breaks) %>%
    select(unit, time, outcome)
  
  # Get true breaks
  true_breaks <- original_data$break_info %>%
    mutate(step_size_val = step_size)
  
  # Get detected breaks
  detected_for_this_size <- primary_detected_breaks %>%
    filter(step_size == !!step_size, unit %in% units_with_breaks) %>%
    group_by(algorithm, unit, time_within_unit) %>%
    summarise(
      avg_probability = mean(probability, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Create focused plot
  p_focused <- ggplot(ts_data_subset, aes(x = time, y = outcome)) +
    geom_line(color = "#34495e", size = 1.2, alpha = 0.8) +
    facet_wrap(~unit, scales = "free_y", ncol = min(3, length(units_with_breaks)),
               labeller = labeller(unit = function(x) paste("Unit", x, "(True Break)"))) +
    
    # True breaks
    geom_vline(data = true_breaks, 
               aes(xintercept = time), 
               color = "#e74c3c", size = 2, alpha = 0.9) +
    
    # Algorithm detections with different styles
    {if (nrow(detected_for_this_size %>% filter(algorithm == "jenkins")) > 0)
      geom_vline(data = detected_for_this_size %>% filter(algorithm == "jenkins"),
                 aes(xintercept = time_within_unit), 
                 color = "#2c3e50", size = 1.5, alpha = 0.8, linetype = "dashed")
    } +
    
    {if (nrow(detected_for_this_size %>% filter(algorithm == "newton")) > 0)
      geom_vline(data = detected_for_this_size %>% filter(algorithm == "newton"),
                 aes(xintercept = time_within_unit), 
                 color = "#f39c12", size = 1.5, alpha = 0.8, linetype = "dotted")
    } +
    
    {if (nrow(detected_for_this_size %>% filter(algorithm == "aberth")) > 0)
      geom_vline(data = detected_for_this_size %>% filter(algorithm == "aberth"),
                 aes(xintercept = time_within_unit), 
                 color = "#3498db", size = 1.5, alpha = 0.8, linetype = "longdash")
    } +
    
    scale_x_continuous(breaks = seq(0, 30, by = 5)) +
    labs(
      title = paste("Break Detection Focus - Step Size", paste0(step_size * 100, "%")),
      subtitle = paste("Units with true breaks only |",
                      "True (red solid), Jenkins (dark dashed), Newton (orange dotted), Aberth (blue long-dash)"),
      x = "Time Period",
      y = "Outcome Value"
    ) +
    theme_minimal() +
    theme(
      panel.grid.major = element_line(color = "#f8f9fa", size = 0.4),
      panel.grid.minor = element_blank(),
      axis.text = element_text(size = 10),
      axis.title = element_text(size = 11),
      plot.title = element_text(size = 14, face = "bold", margin = margin(b = 5)),
      plot.subtitle = element_text(size = 11, color = "#666666", margin = margin(b = 15)),
      strip.text = element_text(size = 10, face = "bold"),
      plot.margin = margin(20, 20, 20, 20)
    )
  
  print(p_focused)
  
  # Print detailed break information
  cat("Detailed break analysis for step size", step_size, ":\n")
  for (unit in units_with_breaks) {
    true_break_time <- true_breaks$time[true_breaks$unit == unit]
    cat(sprintf("  Unit %d - True break at time %d:\n", unit, true_break_time))
    
    unit_detections <- detected_for_this_size %>% filter(unit == !!unit)
    if (nrow(unit_detections) > 0) {
      for (i in 1:nrow(unit_detections)) {
        det <- unit_detections[i, ]
        time_diff <- det$time_within_unit - true_break_time
        cat(sprintf("    %s: detected at time %d (diff: %+d, prob: %.3f)\n", 
                    str_to_title(det$algorithm), det$time_within_unit, 
                    time_diff, det$avg_probability))
      }
    } else {
      cat("    No high-probability detections\n")
    }
  }
  cat("\n")
}
```

```{r}
# Create a summary table of detection accuracy
detection_accuracy_table <- data.frame()

for (step_size in step_sizes) {
  dataset_key <- paste0("step_", step_size)
  original_data <- original_datasets[[dataset_key]]
  
  if (is.null(original_data)) next
  
  true_breaks <- original_data$break_info
  
  for (algorithm in c("jenkins", "newton", "aberth")) {
    detected <- primary_detected_breaks %>%
      filter(step_size == !!step_size, algorithm == !!algorithm) %>%
      group_by(unit, time_within_unit) %>%
      summarise(avg_prob = mean(probability), .groups = "drop") %>%
      arrange(desc(avg_prob)) %>%
      slice(1)  # Take the most likely detection
    
    if (nrow(detected) > 0 && nrow(true_breaks) > 0) {
      # Find closest match
      best_match <- true_breaks[1, ]  # For simplicity, assume one break per dataset
      time_error <- detected$time_within_unit[1] - best_match$time
      unit_match <- detected$unit[1] == best_match$unit
      
      detection_accuracy_table <- rbind(detection_accuracy_table, data.frame(
        step_size = step_size,
        algorithm = algorithm,
        detected_unit = detected$unit[1],
        true_unit = best_match$unit,
        detected_time = detected$time_within_unit[1],
        true_time = best_match$time,
        time_error = time_error,
        unit_correct = unit_match,
        detection_probability = detected$avg_prob[1],
        stringsAsFactors = FALSE
      ))
    }
  }
}

print(kable(detection_accuracy_table %>%
              mutate(across(where(is.numeric), ~round(.x, 3))),
            format = "html", 
            caption = "Break detection accuracy summary"))

cat("\nBreak Detection Performance Summary:\n")
cat("===================================\n")
accuracy_summary <- detection_accuracy_table %>%
  group_by(algorithm) %>%
  summarise(
    avg_time_error = mean(abs(time_error), na.rm = TRUE),
    perfect_detections = sum(time_error == 0 & unit_correct, na.rm = TRUE),
    total_detections = n(),
    accuracy_rate = perfect_detections / total_detections,
    .groups = "drop"
  )

for (i in 1:nrow(accuracy_summary)) {
  alg <- accuracy_summary$algorithm[i]
  cat(sprintf("%s: %.1f avg time error, %d/%d perfect detections (%.1f%%)\n",
              str_to_title(alg),
              accuracy_summary$avg_time_error[i],
              accuracy_summary$perfect_detections[i],
              accuracy_summary$total_detections[i],
              accuracy_summary$accuracy_rate[i] * 100))
}
```

```{r}
# Focus on PIPs (Posterior Inclusion Probabilities) for break indicators
pip_data <- step_size_estimates_clean %>%
  filter(parameter_type == "break_indicator") %>%
  mutate(
    # Calculate break position in the panel structure
    break_indicators_per_unit = 28,  # t-2 for t=30
    unit = floor(parameter_index / break_indicators_per_unit) + 1,
    time_within_unit = (parameter_index %% break_indicators_per_unit) + 2,
    step_size_pct = paste0(step_size * 100, "%")
  ) %>%
  select(algorithm, step_size, step_size_pct, dataset_name, run_number, 
         unit, time_within_unit, parameter_index, probability)

cat("PIP data prepared:\n")
cat("  Break indicators per dataset:", max(pip_data$parameter_index) + 1, "\n")
cat("  Units:", paste(sort(unique(pip_data$unit)), collapse = ", "), "\n")
cat("  Time range:", min(pip_data$time_within_unit), "to", max(pip_data$time_within_unit), "\n")
```

```{r}
# Calculate PIP differences between algorithms for each break size
pip_comparison <- pip_data %>%
  # Average PIPs across runs for each algorithm/step_size/position combination
  group_by(algorithm, step_size, step_size_pct, unit, time_within_unit, parameter_index) %>%
  summarise(avg_pip = mean(probability, na.rm = TRUE), .groups = "drop") %>%
  
  # Pivot to compare algorithms side by side
  pivot_wider(names_from = algorithm, values_from = avg_pip, names_prefix = "pip_") %>%
  
  # Calculate differences from Jenkins baseline
  mutate(
    newton_diff = pip_newton - pip_jenkins,
    aberth_diff = pip_aberth - pip_jenkins,
    newton_abs_diff = abs(newton_diff),
    aberth_abs_diff = abs(aberth_diff)
  ) %>%
  filter(!is.na(pip_jenkins), !is.na(pip_newton), !is.na(pip_aberth))

# Summary statistics for PIP differences by step size
pip_diff_summary <- pip_comparison %>%
  group_by(step_size, step_size_pct) %>%
  summarise(
    n_indicators = n(),
    # Newton vs Jenkins
    newton_mean_abs_diff = mean(newton_abs_diff, na.rm = TRUE),
    newton_max_abs_diff = max(newton_abs_diff, na.rm = TRUE),
    newton_mean_diff = mean(newton_diff, na.rm = TRUE),
    # Aberth vs Jenkins  
    aberth_mean_abs_diff = mean(aberth_abs_diff, na.rm = TRUE),
    aberth_max_abs_diff = max(aberth_abs_diff, na.rm = TRUE),
    aberth_mean_diff = mean(aberth_diff, na.rm = TRUE),
    # High PIP indicators (>0.1) - these matter most
    high_pip_indicators = sum(pip_jenkins > 0.1, na.rm = TRUE),
    newton_high_pip_diff = ifelse(high_pip_indicators > 0,
                                  mean(newton_abs_diff[pip_jenkins > 0.1], na.rm = TRUE), NA),
    aberth_high_pip_diff = ifelse(high_pip_indicators > 0,
                                  mean(aberth_abs_diff[pip_jenkins > 0.1], na.rm = TRUE), NA),
    .groups = "drop"
  )

print(kable(pip_diff_summary %>%
              mutate(across(where(is.numeric), ~round(.x, 4))),
            format = "html", 
            caption = "PIP Differences between Root Finding Algorithms by Break Size"))
```

```{r}
# Plot PIP distributions for each algorithm and step size
pip_long <- pip_data %>%
  group_by(algorithm, step_size, step_size_pct, parameter_index) %>%
  summarise(avg_pip = mean(probability, na.rm = TRUE), .groups = "drop") %>%
  mutate(algorithm_factor = factor(algorithm, levels = c("jenkins", "newton", "aberth")))

ggplot(pip_long, aes(x = avg_pip, fill = algorithm_factor)) +
  geom_histogram(bins = 50, alpha = 0.7, position = "identity") +
  facet_grid(step_size_pct ~ algorithm_factor, 
             labeller = labeller(step_size_pct = function(x) paste("Step Size:", x))) +
  scale_fill_manual(
    values = c("jenkins" = "#2c3e50", "newton" = "#e74c3c", "aberth" = "#3498db"),
    labels = c("Jenkins", "Newton-Raphson", "Aberth-Ehrlich")
  ) +
  scale_x_continuous(
    limits = c(0, 1),
    breaks = c(0, 0.25, 0.5, 0.75, 1.0),
    labels = c("0%", "25%", "50%", "75%", "100%")
  ) +
  labs(
    title = "Distribution of Posterior Inclusion Probabilities (PIPs)",
    subtitle = "Comparing root finding algorithms across break sizes",
    x = "Posterior Inclusion Probability",
    y = "Count of Break Indicators",
    fill = "Algorithm"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "#f8f9fa", size = 0.3),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 9),
    axis.title = element_text(size = 10),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11, color = "#666666"),
    strip.text = element_text(size = 9),
    legend.position = "bottom"
  )
```

```{r}
# Create heatmap showing PIP differences for high-probability indicators
high_pip_data <- pip_comparison %>%
  filter(pip_jenkins > 0.05) %>%  # Focus on indicators that Jenkins gives some weight to
  select(step_size_pct, unit, time_within_unit, pip_jenkins, newton_diff, aberth_diff) %>%
  pivot_longer(cols = c(newton_diff, aberth_diff), 
               names_to = "comparison", values_to = "pip_difference") %>%
  mutate(
    comparison = case_when(
      comparison == "newton_diff" ~ "Newton vs Jenkins",
      comparison == "aberth_diff" ~ "Aberth vs Jenkins"
    ),
    position_label = paste("U", unit, "T", time_within_unit)
  )

ggplot(high_pip_data, aes(x = step_size_pct, y = position_label, fill = pip_difference)) +
  geom_tile(color = "white", size = 0.5) +
  facet_wrap(~comparison, ncol = 1) +
  scale_fill_gradient2(
    low = "#3498db", mid = "white", high = "#e74c3c",
    midpoint = 0, 
    limits = c(-0.1, 0.1),
    labels = scales::percent_format(accuracy = 1),
    name = "PIP\nDifference"
  ) +
  labs(
    title = "PIP Differences for High-Probability Break Indicators",
    subtitle = "Only showing break indicators where Jenkins PIP > 5%",
    x = "Break Size (Step Mean)",
    y = "Break Position (Unit, Time)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    axis.text.y = element_text(size = 8),
    axis.title = element_text(size = 10),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11, color = "#666666"),
    strip.text = element_text(size = 10, face = "bold"),
    legend.title = element_text(size = 9),
    panel.grid = element_blank()
  )
```

```{r}
# Scatter plots comparing PIPs directly between algorithms
pip_scatter_data <- pip_comparison %>%
  select(step_size_pct, pip_jenkins, pip_newton, pip_aberth) %>%
  pivot_longer(cols = c(pip_newton, pip_aberth), 
               names_to = "algorithm", values_to = "pip_alternative") %>%
  mutate(
    algorithm = case_when(
      algorithm == "pip_newton" ~ "Newton-Raphson",
      algorithm == "pip_aberth" ~ "Aberth-Ehrlich"
    )
  )

ggplot(pip_scatter_data, aes(x = pip_jenkins, y = pip_alternative)) +
  geom_point(alpha = 0.6, size = 1.5, color = "#34495e") +
  geom_abline(slope = 1, intercept = 0, color = "#e74c3c", linetype = "dashed", size = 1) +
  facet_grid(algorithm ~ step_size_pct,
             labeller = labeller(step_size_pct = function(x) paste("Step Size:", x))) +
  scale_x_continuous(
    limits = c(0, 1),
    labels = scales::percent_format(accuracy = 1)
  ) +
  scale_y_continuous(
    limits = c(0, 1),
    labels = scales::percent_format(accuracy = 1)
  ) +
  labs(
    title = "PIP Correlation between Root Finding Algorithms",
    subtitle = "Perfect agreement would lie on the red diagonal line",
    x = "Jenkins PIP",
    y = "Alternative Algorithm PIP"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_line(color = "#f8f9fa", size = 0.3),
    panel.grid.minor = element_blank(),
    axis.text = element_text(size = 9),
    axis.title = element_text(size = 10),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11, color = "#666666"),
    strip.text = element_text(size = 9)
  )
```

```{r}
# Calculate correlation and agreement metrics
pip_correlations <- pip_comparison %>%
  group_by(step_size, step_size_pct) %>%
  summarise(
    jenkins_newton_cor = cor(pip_jenkins, pip_newton, use = "complete.obs"),
    jenkins_aberth_cor = cor(pip_jenkins, pip_aberth, use = "complete.obs"),
    # Root Mean Square Difference
    newton_rmsd = sqrt(mean(newton_diff^2, na.rm = TRUE)),
    aberth_rmsd = sqrt(mean(aberth_diff^2, na.rm = TRUE)),
    # Agreement within 1% for high PIPs
    newton_agreement_1pct = mean(newton_abs_diff < 0.01, na.rm = TRUE),
    aberth_agreement_1pct = mean(aberth_abs_diff < 0.01, na.rm = TRUE),
    .groups = "drop"
  )

print(kable(pip_correlations %>%
              mutate(across(where(is.numeric), ~round(.x, 4))),
            format = "html",
            caption = "PIP Agreement Metrics between Root Finding Algorithms"))

cat("\n=== PIP ANALYSIS SUMMARY ===\n")
cat("============================\n\n")

cat("POSTERIOR INCLUSION PROBABILITY (PIP) DIFFERENCES:\n\n")

for(i in 1:nrow(pip_correlations)) {
  step <- pip_correlations$step_size_pct[i]
  newton_cor <- round(pip_correlations$jenkins_newton_cor[i], 4)
  aberth_cor <- round(pip_correlations$jenkins_aberth_cor[i], 4)
  newton_rmsd <- round(pip_correlations$newton_rmsd[i], 4)
  aberth_rmsd <- round(pip_correlations$aberth_rmsd[i], 4)
  
  cat(sprintf("Step Size %s:\n", step))
  cat(sprintf("  Newton-Raphson: r=%.4f, RMSD=%.4f\n", newton_cor, newton_rmsd))
  cat(sprintf("  Aberth-Ehrlich: r=%.4f, RMSD=%.4f\n", aberth_cor, aberth_rmsd))
  
  if(newton_cor > 0.999 && aberth_cor > 0.999) {
    cat("  → Nearly identical PIPs across algorithms\n")
  } else if(min(newton_cor, aberth_cor) > 0.99) {
    cat("  → Very high agreement, minor differences\n")
  } else {
    cat("  → Notable differences in PIPs\n")
  }
  cat("\n")
}

# Overall conclusion
overall_newton_cor <- mean(pip_correlations$jenkins_newton_cor, na.rm = TRUE)
overall_aberth_cor <- mean(pip_correlations$jenkins_aberth_cor, na.rm = TRUE)

cat("OVERALL CONCLUSION:\n")
cat(sprintf("Average correlations: Newton r=%.4f, Aberth r=%.4f\n", 
            overall_newton_cor, overall_aberth_cor))

if(min(overall_newton_cor, overall_aberth_cor) > 0.999) {
  cat("The root finding algorithms produce virtually identical PIPs.\n")
  cat("Choice of algorithm should be based on computational efficiency.\n")
} else {
  cat("There are measurable differences in PIPs between algorithms.\n")
  cat("This suggests the root finding step affects the final model selection.\n")
}
```